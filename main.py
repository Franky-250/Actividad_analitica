# -*- coding: utf-8 -*-
"""Actividad_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Franky-250/Actividad_analitica/blob/main/Actividad_2.ipynb

<div style="background-color: #ffffff; color: #000000; padding: 10px; font-family: Arial, sans-serif; line-height: 1.5;">
<div style="text-align: center; color: #000000;">
    <h1>Evidencia de Aprendizaje 2</h1>
    <p>Optimizando la productividad en el mundo del software</p>
    <hr style="border: none; border-top: 1px solid #000000; width: 90%; margin: 25px auto;">
</div>
<div style="margin: 0 auto; width:60%;">
<table style="width: 100%; border-collapse: collapse; color: #000000;">
  <tr style="background-color: #ffffff;">
    <th style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Materia</th>
    <td style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Programación para Análisis de Datos</td>
  </tr>
  <tr style="background-color: #ffffff;">
    <th style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Grupo</th>
    <td style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">PREICA2402B020101</td>
  </tr>
  <tr style="background-color: #ffffff;">
    <th style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Docente</th>
    <td style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Andrés Felipe Palacio</td>
  </tr>
  <tr style="background-color: #ffffff;">
    <th style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Alumnos</th>
    <td style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Wilson Emilio David - Lida Cristina Florez - Eduin Echeverry Hurtado - Jesus Manuel Segura</td>
  </tr>
  <tr style="background-color: #ffffff;">
    <th style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Fecha</th>
    <td style="text-align: left; padding: 8px; border-bottom: 1px solid #000000;">Noviembre de 2024</td>
  </tr>
</table>
</div>
</div>

## **Introducción**

<p align="justify">
En este proyecto nos propusimos demostrar cómo el uso de herramientas modernas como Git y GitHub pueden facilitar la colaboración y organización en el desarrollo de software. A través de un enfoque práctico y en equipo, trabajamos en un proyecto que combina la extracción de datos de Mercado Libre mediante scraping, su análisis y representación gráfica, todo gestionado dentro de un flujo de trabajo DevOps.
</p>

<p align="justify">
El objetivo no solo fue realizar un scraping eficiente, sino también aprender a integrar un control de versiones estructurado, colaborativo y automatizado. Además, implementamos un pipeline de integración continua y despliegue continuo (CI/CD) para garantizar que los cambios realizados por los integrantes del equipo sean seguros y confiables antes de ser aprobados.
</p>

<p align="justify">
Esta experiencia no solo nos ayudó a fortalecer habilidades técnicas, sino también a entender la importancia de trabajar en equipo, planificar tareas y construir soluciones que se adapten a las necesidades actuales del desarrollo de software. En pocas palabras, el proyecto es una ventana al mundo real de la productividad y el trabajo en equipo en el ámbito del desarrollo tecnológico.
</p>

## **Descripción de la página y artículo a analizar**

<p align="justify">
Mercado Libre es una de las plataformas de comercio electrónico más grandes y reconocidas a nivel mundial. Con un catálogo inmenso que abarca desde productos electrónicos hasta ropa y accesorios deportivos, la plataforma ofrece una experiencia de compra accesible, con múltiples opciones de pago y un sistema de envío confiable. Mercado Libre permite a sus usuarios revisar y comparar productos de diversas categorías, así como visualizar reseñas y opiniones de otros compradores, lo cual mejora la toma de decisiones de compra.
</p>

<p align="justify">
Para esta actividad de scraping, se ha seleccionado el calzado deportivo Nike para hombre como el artículo de interés dentro de la categoría de ropa y accesorios deportivos. La popularidad de estos artículos, tanto por su calidad como por el reconocimiento de la marca Nike, convierte al calzado deportivo en un producto ideal para este análisis. El objetivo de la extracción de datos es obtener información relevante, como títulos y precios de varios modelos de calzado Nike disponibles en Mercado Libre, con el fin de analizar la oferta y posibles tendencias de precios en el mercado de calzado deportivo masculino. Estos datos pueden resultar valiosos para estudios de mercado, especialmente en la evaluación de la competitividad de precios y el análisis de productos similares en diferentes rangos de precio.
</p>

## **Descripción del tema de interés que deseamos desarrollar en la primera práctica**

<p align="justify">
En esta primera práctica, nuestro equipo se enfocó en explorar el mercado de calzado deportivo masculino de la marca Nike en Mercado Libre. Dado que Mercado Libre es una de las plataformas de comercio electrónico más grandes y populares, resulta un sitio ideal para analizar cómo se presentan estos productos, los rangos de precios que manejan y los modelos que captan mayor atención. Como equipo, decidimos aplicar técnicas de scraping para extraer información clave de varios modelos de tenis Nike, como sus nombres y precios, de manera automatizada.
</p>

<p align="justify">
Este análisis en conjunto nos permite obtener una visión más amplia del mercado de calzado deportivo en línea y detectar patrones de precios, diferencias entre modelos y estrategias de presentación que Mercado Libre utiliza. Nuestra intención es identificar oportunidades y tendencias de precio que puedan ser útiles para consumidores, así como para quienes estén interesados en el mercado deportivo. Al realizar esta actividad de forma colaborativa, buscamos mejorar nuestra comprensión del mercado y las habilidades en el manejo de herramientas de extracción de datos.
</p>

## **Objetivo**

<p align="justify">
Implementar un flujo de trabajo colaborativo utilizando herramientas como Git, GitHub y GitHub Actions, para gestionar versiones y automatizar procesos en un proyecto de análisis de datos obtenido mediante técnicas de scraping en la plataforma Mercado Libre.
</p>

<h3><strong>Objetivos Específicos</strong></h3>

<ul>
    <li align="justify">Utilizar Git y GitHub como herramientas para gestionar versiones y coordinar aportes entre los integrantes del equipo de forma eficiente.</li>
    <li align="justify">Implementar técnicas de scraping con BeautifulSoup para extraer datos relevantes de productos, como títulos y precios, desde la plataforma Mercado Libre.</li>
    <li align="justify">Organizar y almacenar los datos extraídos en formatos estructurados como CSV y JSON, asegurando su reutilización y portabilidad en diferentes entornos.</li>
    <li align="justify">Crear visualizaciones gráficas claras e intuitivas, como gráficos de barras y gráficos de torta, que permitan analizar patrones y tendencias en los datos recopilados.</li>
    <li align="justify">Establecer un pipeline de CI/CD con GitHub Actions para integrar pruebas automatizadas y garantizar la calidad del código antes de su aprobación.</li> <li align="justify">Documentar el flujo de trabajo DevOps implementado en un repositorio público de GitHub, proporcionando una guía completa y comprensible para posibles colaboradores.</li> </ul>
</ul>

## **Metodología empleada de scraping**

<p style="text-align: justify;">
En la Evidencia de Aprendizaje 2, retomamos el proyecto iniciado en la Evidencia de Aprendizaje 1, donde se desarrollaron las bases para extraer datos de productos en Mercado Libre utilizando BeautifulSoup. En esta actividad, se amplió el alcance del proyecto implementando un flujo de trabajo colaborativo en GitHub con control de versiones y prácticas de DevOps, optimizando la eficiencia y la calidad del proceso. A continuación, se describe la metodología empleada:
</p>

<ol style="text-align: justify;">
   <li style="text-align: justify;">
       <strong>Integración de Git y GitHub en el flujo de trabajo:</strong>
       Se creó un repositorio en GitHub para gestionar de manera eficiente el proyecto. Cada colaborador realizó un fork del repositorio principal y trabajó en su propia rama para agregar funcionalidades específicas, como la generación de gráficos y el análisis de datos. Se implementaron pull requests para la validación y aprobación de los cambios antes de ser fusionados en la rama principal.
   </li>
   
   <li style="text-align: justify;">
       <strong>Extensión del scraping realizado en la Actividad 1:</strong>
       Retomando la extracción de datos sobre calzado deportivo Nike en Mercado Libre, se optimizó el proceso inicial, asegurando que los datos recopilados fueran más completos y relevantes. La configuración de los headers y la implementación de pausas en las solicitudes (<code>time.sleep(8)</code>) garantizaron un scraping más seguro y estable.
   </li>

   <li style="text-align: justify;">
       <strong>Almacenamiento y estructura de los datos:</strong>
       Los datos extraídos, que incluían títulos y precios de productos, se almacenaron en múltiples formatos:
       <ul style="text-align: justify;">
           <li style="text-align: justify;"><strong>CSV:</strong> Permitiendo un análisis más detallado con herramientas como pandas.</li>
           <li style="text-align: justify;"><strong>JSON:</strong> Asegurando la interoperabilidad para futuros usos en aplicaciones web.</li>
           <li style="text-align: justify;"><strong>HTML:</strong> Generando un catálogo de productos que presenta la información extraída en un formato visualmente atractivo.</li>
       </ul>
   </li>

   <li style="text-align: justify;">
       <strong>Implementación de nuevas funcionalidades:</strong>
       En la Actividad 2, los colaboradores contribuyeron con nuevas funcionalidades, como:
       <ul style="text-align: justify;">
           <li style="text-align: justify;">Gráficos de barras que muestran los 10 productos más caros en orden descendente.</li>
           <li style="text-align: justify;">Gráficos de torta que ilustran la distribución porcentual de los precios.</li>
           <li style="text-align: justify;">Procesamiento de los datos en un DataFrame de pandas para facilitar la manipulación y visualización tabular.</li>
       </ul>
       Estas funcionalidades se integraron mediante el flujo de trabajo colaborativo en GitHub, asegurando una revisión y validación de cada cambio.
   </li>

   <li style="text-align: justify;">
       <strong>Automatización con GitHub Actions:</strong>
       Se estableció un pipeline de CI/CD para garantizar la calidad del código. Las pruebas automatizadas verificaron que las modificaciones de los colaboradores fueran correctas antes de ser fusionadas. Este flujo de trabajo contribuyó a la integración continua y al despliegue seguro de los resultados.
   </li>

   <li style="text-align: justify;">
       <strong>Manejo de errores y evaluación de herramientas:</strong>
       Se implementaron controles para manejar posibles errores durante el scraping, como solicitudes fallidas o cambios en la estructura de la página. Aunque BeautifulSoup fue la herramienta principal, se consideraron alternativas como Selenium o Scrapy para futuros proyectos que involucren sitios más dinámicos.
   </li>
</ol>

<p style="text-align: justify;">
Este enfoque metodológico permitió extender el trabajo realizado en la Evidencia de Aprendizaje 1 hacia un desarrollo más robusto, integrando buenas prácticas de colaboración, control de versiones y análisis de datos, lo que resultó en un proyecto más completo y profesional.
</p>

## **Resultados**

<p align="justify">
Los resultados obtenidos en esta actividad reflejan la efectividad de la metodología aplicada en el scraping, así como en la implementación del flujo de trabajo colaborativo en GitHub. Se logró extraer información detallada de productos en Mercado Libre, específicamente de calzado deportivo masculino de la marca Nike, incluyendo el nombre de cada producto y su precio, de forma exitosa y sin interrupciones técnicas.
</p>

<p align="justify">
Además, los datos extraídos se organizaron y almacenaron en formatos accesibles para diferentes propósitos:
</p>

<ul align="justify">
    <li style="text-align: justify;"><strong>Archivo CSV:</strong> Este formato facilitó el análisis estructurado en herramientas como pandas o Excel, proporcionando una tabla clara y organizada de productos y precios.</li>
    <li style="text-align: justify;"><strong>Archivo JSON:</strong> Este formato fue útil para integraciones con aplicaciones web o proyectos que requieren datos estructurados en un lenguaje de programación.</li>
    <li style="text-align: justify;"><strong>Archivo HTML:</strong> Generamos un archivo que presenta la información de manera visual y ordenada, simulando un catálogo de productos en línea que destaca los títulos y precios de los productos.</li>
</ul>

<p align="justify">
En esta actividad, los colaboradores también agregaron nuevas funcionalidades mediante gráficos:
</p>

<ul align="justify">
    <li style="text-align: justify;">Un gráfico de barras que muestra los 10 productos más caros, permitiendo identificar de manera rápida las tendencias en los precios.</li>
    <li style="text-align: justify;">Un gráfico de torta que representa la distribución porcentual de los precios de estos 10 productos, proporcionando una perspectiva visual sobre cómo se dividen los costos.</li>
    <li style="text-align: justify;">Procesamiento de datos en un DataFrame de pandas para facilitar su manejo y análisis tabular.</li>
</ul>

<p align="justify">
Por último, el uso de GitHub Actions permitió implementar un pipeline de CI/CD que automatizó la validación del código y garantizó que las modificaciones realizadas por los colaboradores fueran seguras antes de fusionarlas. Este proceso optimizó la calidad y consistencia del proyecto final.
</p>

<p align="justify">
En general, los resultados obtenidos no solo validan la estrategia utilizada, sino que también resaltan la importancia de la colaboración y las herramientas modernas en el desarrollo de software, logrando un producto completo y profesional.
</p>

#**Código**
"""

!git clone https://github.com/jesussegura45/Actividad_analitica.git

"""Este comando clona el repositorio Actividad_analitica desde GitHub a tu entorno de trabajo (local o virtual, como Google Colab). Descarga todos los archivos, carpetas y el historial del repositorio.

Importante para colaboradores:
Cada colaborador debe reemplazar la URL por la ruta de su propio repositorio
"""

# **************************************************
# Librerias utilizadas
# **************************************************

import requests
from bs4 import BeautifulSoup
import os
import time
import csv
import json
import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Este bloque importa todas las librerías necesarias para realizar las tareas principales del proyecto, como el scraping, el análisis y la visualización de datos."""

# **************************************************
# Proceso de Scrape
# **************************************************

def scrape_mercado_libre(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36",
        "Accept-Language": "es-ES, en;q=0.5",
        "Referer": "https://www.mercadolibre.com.co/",
        "Accept-Encoding": "gzip, deflate, br",
    }

    try:
        # Pausa para reducir el riesgo de bloqueo
        time.sleep(8)

        # Realizar la solicitud a la URL
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        resultados = ""
        data_list = []  # Lista para almacenar datos para CSV y JSON

        # Extraer los productos
        productos = soup.find_all('h2', class_='poly-box poly-component__title')

        for producto in productos:
            # Extraer el título
            titulo_elemento = producto.find('a')
            titulo = titulo_elemento.text.strip() if titulo_elemento else "Título no encontrado"

            # Extraer el precio
            precio_elemento = producto.find_next('span', class_='andes-money-amount__fraction')
            precio = precio_elemento.text.strip() if precio_elemento else "Precio no encontrado"

            resultados += f"Título: {titulo}\n"
            resultados += f"Precio: {precio}\n"
            resultados += f"_ _ _\n"
            data_list.append({"Título": titulo, "Precio": precio})

        # Guardar el HTML
        output_dir = 'content/sample_data'
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        with open(f'{output_dir}/mercado_libre_products.html', 'w', encoding="utf-8") as file:
            file.write(str(soup))

        # Guardar datos en archivo CSV
        with open(f'{output_dir}/mercado_libre_products.csv', 'w', newline='', encoding="utf-8") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=["Título", "Precio"])
            writer.writeheader()
            writer.writerows(data_list)

        # Guardar datos en archivo JSON
        with open(f'{output_dir}/mercado_libre_products.json', 'w', encoding="utf-8") as json_file:
            json.dump(data_list, json_file, ensure_ascii=False, indent=4)

        return resultados

    except requests.exceptions.RequestException as e:
        return f"Error al hacer la solicitud: {e}"
    except AttributeError as e:
        return f"Error al analizar el HTML: {e}. Verificar el selector CSS."
    except Exception as e:
        return f"Error inesperado: {e}"

# URL de Mercado Libre
url_mercado_libre = "https://listado.mercadolibre.com.co/tenis-nike-hombre"
resultados = scrape_mercado_libre(url_mercado_libre)
print(resultados)

"""Este código define la función scrape_mercado_libre que realiza el scraping de datos en Mercado Libre, obteniendo información sobre productos como títulos y precios."""

# **************************************************
# Gráfico
# **************************************************

# Cargar los datos del archivo JSON generado
with open('content/sample_data/mercado_libre_products.json', 'r', encoding="utf-8") as json_file:
    productos = json.load(json_file)

# Procesar los datos para extraer títulos y precios
datos_filtrados = []
for producto in productos:
    titulo = producto.get("Título", "Título no encontrado")
    precio = producto.get("Precio", "0").replace(".", "").replace(",", "")
    try:
        precio = float(precio)  # Convertir a número
        datos_filtrados.append({"Título": titulo, "Precio": precio})
    except ValueError:
        continue  # Ignorar precios no válidos

# Ordenar por precio en orden descendente y seleccionar los 10 más caros
top_10_tenis = sorted(datos_filtrados, key=lambda x: x["Precio"], reverse=True)[:10]

# Extraer los títulos y precios para el gráfico
titulos_top = [producto["Título"] for producto in top_10_tenis]
precios_top = [producto["Precio"] for producto in top_10_tenis]

# Crear el gráfico de barras
plt.figure(figsize=(12, 6))
sns.barplot(x=precios_top, y=titulos_top, palette="magma")

plt.title("Top 10 Tenis Más Caros en Mercado Libre", fontsize=16)
plt.xlabel("Precio (COP)", fontsize=14)
plt.ylabel("Producto", fontsize=14)
plt.tight_layout()

# Mostrar el gráfico
plt.show()

"""Este bloque de código se encarga de procesar los datos extraídos mediante scraping y visualizarlos en un gráfico de barras, mostrando los 10 productos más caros junto con sus precios."""

# Calcular el total de los precios
total_precio = sum(precios_top)

# Calcular los porcentajes
porcentajes = [(precio / total_precio) * 100 for precio in precios_top]

# Crear el gráfico de torta
plt.figure(figsize=(10, 8))
plt.pie(
    porcentajes,
    labels=titulos_top,
    autopct='%1.1f%%',
    startangle=140,
    colors=sns.color_palette("magma", len(precios_top))
)

plt.title("Distribución Porcentual de los 10 Tenis Más Caros en Mercado Libre", fontsize=16)
plt.tight_layout()

# Mostrar el gráfico
plt.show()

"""En esta celda se calcula el total de los precios de los 10 tenis más caros al sumar todos los valores en la lista precios_top. Luego, utiliza este total para calcular los porcentajes que cada precio representa respecto al total, dividiendo cada precio entre el total y multiplicándolo por 100."""

# Llamar a la función para obtener los resultados
url_mercado_libre = "https://listado.mercadolibre.com.co/tenis-nike-hombre"
resultados = scrape_mercado_libre(url_mercado_libre)

# Procesar los resultados para extraer los productos
productos = []
for linea in resultados.split("_ _ _"):
    if "Título:" in linea and "Precio:" in linea:
        partes = linea.split("\n")
        titulo = partes[0].replace("Título: ", "").strip()
        precio = partes[1].replace("Precio: ", "").strip()
        productos.append({"Título": titulo, "Precio": precio})

# Crear un DataFrame con pandas y mostrar los primeros 10 productos
import pandas as pd
df_productos = pd.DataFrame(productos[:10])

# Mostrar el DataFrame
from IPython.display import display
display(df_productos)

"""A partir del gráfico de barras se puede observar los 10 tenis mas caros. El tenis mas costoso corresponde a Tenis Jordan Mvp Remix-negro, cuyo valor es de $934.990.

#**Conclusiones**

<p align="justify">
Esta actividad permitió consolidar conocimientos sobre scraping web y la importancia de la colaboración en proyectos de desarrollo. Utilizando <strong>BeautifulSoup</strong> como herramienta principal, logramos extraer datos clave de Mercado Libre, como títulos y precios de productos, demostrando que es una opción eficiente para sitios con HTML estático.
</p>

<p align="justify">
Configurar adecuadamente los headers y añadir pausas entre las solicitudes fueron pasos esenciales para evitar bloqueos y garantizar un scraping fluido. Al almacenar los datos en múltiples formatos como <strong>CSV</strong>, <strong>JSON</strong> y <strong>HTML</strong>, se facilitó su análisis y reutilización en diferentes plataformas, mientras que las visualizaciones generadas en gráficos de barras y torta hicieron más intuitiva la interpretación de los resultados.
</p>

<p align="justify">
El uso de <strong>GitHub</strong> y <strong>GitHub Actions</strong> fue clave para el flujo de trabajo colaborativo, asegurando la calidad del código a través de pruebas automatizadas y facilitando la integración continua. Esto no solo mejoró la organización del equipo, sino que también optimizó la eficiencia y la gestión del proyecto.
</p>

<p align="justify">
En resumen, esta experiencia fue altamente enriquecedora, proporcionando una base sólida en técnicas de scraping, manejo de datos y prácticas de colaboración en desarrollo de software, además de aportar un enfoque práctico y profesional para futuros proyectos.
</p>

# **Bibliografía**

<ul style="text-align: justify; list-style-type: disc;">
    <li>
        <p align="justify">
            books.toscrape. (2024). <em>Books to Scrape</em>. <a href="https://books.toscrape.com/">https://books.toscrape.com/</a>
        </p>
    </li>
    <li>
        <p align="justify">
            Clemente, D., Sinsajoa, G., Sebastian, A., Quintero, T., Esteban, D., & Estrella, F. (2024). Desarrollo de una herramienta de adquisición automática de datos de fuentes externas, para el sistema de gestión de información de la Vicerrectoría de [Doctoral dissertation, Universidad CESMAG]. <a href="http://repositorio.unicesmag.edu.co:8080/jspui/handle/123456789/1175">http://repositorio.unicesmag.edu.co:8080/jspui/handle/123456789/1175</a>
        </p>
    </li>
    <li>
        <p align="justify">
            Mijangos-Espinosa, R., Martínez-Rebollar, A., Estrada-Esquivel, H., & Hernández-Pérez, Y. (2022). Uso de técnicas de Web Scraping para obtención automática de bases de datos en la Web. <em>rcs.cic.ipn.mx</em>, 151(5), 143–157. <a href="https://rcs.cic.ipn.mx/2022_151_5/Uso%20de%20tecnicas%20de%20Web%20Scraping%20para%20obtencion%20automatica%20de%20bases%20de%20datos%20en%20la%20Web.pdf">https://rcs.cic.ipn.mx/2022_151_5/Uso%20de%20tecnicas%20de%20Web%20Scraping%20para%20obtencion%20automatica%20de%20bases%20de%20datos%20en%20la%20Web.pdf</a>
        </p>
    </li>
</ul>
"""